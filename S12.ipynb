{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "346CyjRWfo_y",
        "outputId": "f9b2547b-437b-42e1-97e9-ab664fb86eb2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "1_cR0iX-gxcB",
        "outputId": "4599c51c-e927-423f-97dc-6c7e0fd2013e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd drive/MyDrive/tsai_yolo/YoloV3-master"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7RmpCilga4p",
        "outputId": "b7f814d2-71fd-450c-8e4b-6883f8c592aa"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/tsai_yolo/YoloV3-master\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py --data data/dataset/custom.data --batch 10 --cache --epochs 25 --nosave\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jyr-J-Mmhxb7",
        "outputId": "7614f816-a38b-43cc-cde4-23d605e21e8a"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(epochs=25, batch_size=10, accumulate=4, cfg='cfg/yolov3-spp.cfg', data='data/dataset/custom.data', multi_scale=False, img_size=[512], rect=False, resume=False, nosave=True, notest=False, evolve=False, bucket='', cache_images=True, weights='weights/yolov3-spp-ultralytics.pt', name='', device='', adam=False, single_cls=False)\n",
            "Using CUDA device0 _CudaDeviceProperties(name='Tesla T4', total_memory=15102MB)\n",
            "\n",
            "2024-04-19 19:07:17.891305: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-19 19:07:17.891365: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-19 19:07:17.892880: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-19 19:07:19.056496: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Run 'tensorboard --logdir=runs' to view tensorboard at http://localhost:6006/\n",
            "WARNING: smart bias initialization failure.\n",
            "WARNING: smart bias initialization failure.\n",
            "WARNING: smart bias initialization failure.\n",
            "Model Summary: 225 layers, 6.29987e+07 parameters, 6.29987e+07 gradients\n",
            "Caching labels (101 found, 0 missing, 1 empty, 0 duplicate, for 102 images): 100% 102/102 [00:00<00:00, 358.38it/s]\n",
            "Caching images (0.1GB): 100% 102/102 [00:01<00:00, 78.00it/s]\n",
            "Caching labels (10 found, 0 missing, 1 empty, 0 duplicate, for 11 images): 100% 11/11 [00:00<00:00, 392.73it/s]\n",
            "Caching images (0.0GB): 100% 11/11 [00:00<00:00, 73.31it/s]\n",
            "Image sizes 512 - 512 train, 512 test\n",
            "Using 2 dataloader workers\n",
            "Starting training for 25 epochs...\n",
            "\n",
            "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "  0% 0/11 [00:00<?, ?it/s]/content/drive/MyDrive/tsai_yolo/YoloV3-master/utils/utils.py:374: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n",
            "  lcls, lbox, lobj = ft([0]), ft([0]), ft([0])\n",
            "/usr/local/lib/python3.10/dist-packages/torch/cuda/memory.py:440: FutureWarning: torch.cuda.memory_cached has been renamed to torch.cuda.memory_reserved\n",
            "  warnings.warn(\n",
            "      0/24     7.94G      6.07      3.36     0.242      9.67        18       512:   9% 1/11 [00:05<00:54,  5.41s/it]/usr/local/lib/python3.10/dist-packages/torch/cuda/memory.py:440: FutureWarning: torch.cuda.memory_cached has been renamed to torch.cuda.memory_reserved\n",
            "  warnings.warn(\n",
            "      0/24     7.95G      4.89      3.55     0.184      8.62         2       512: 100% 11/11 [00:13<00:00,  1.18s/it]\n",
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:507: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3549.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "               Class    Images   Targets         P         R   mAP@0.5        F1:   0% 0/2 [00:00<?, ?it/s]/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "               Class    Images   Targets         P         R   mAP@0.5        F1: 100% 2/2 [00:02<00:00,  1.04s/it]\n",
            "                 all        11        20         0         0   0.00241         0\n",
            "\n",
            "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n",
            "      1/24     7.95G      4.58      3.49     0.182      8.25         5       512: 100% 11/11 [00:08<00:00,  1.35it/s]\n",
            "               Class    Images   Targets         P         R   mAP@0.5        F1: 100% 2/2 [00:00<00:00,  3.89it/s]\n",
            "                 all        11        20         0         0   0.00856         0\n",
            "\n",
            "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n",
            "      2/24     8.29G      4.71      3.43     0.192      8.33         5       512: 100% 11/11 [00:08<00:00,  1.31it/s]\n",
            "               Class    Images   Targets         P         R   mAP@0.5        F1: 100% 2/2 [00:00<00:00,  4.58it/s]\n",
            "                 all        11        20         0         0   0.00164         0\n",
            "\n",
            "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n",
            "      3/24     8.29G      4.58      2.76     0.194      7.54         6       512: 100% 11/11 [00:08<00:00,  1.30it/s]\n",
            "               Class    Images   Targets         P         R   mAP@0.5        F1: 100% 2/2 [00:00<00:00,  4.64it/s]\n",
            "                 all        11        20         0         0    0.0118         0\n",
            "\n",
            "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n",
            "      4/24     8.29G      4.55      2.38     0.189      7.12         3       512: 100% 11/11 [00:08<00:00,  1.30it/s]\n",
            "               Class    Images   Targets         P         R   mAP@0.5        F1: 100% 2/2 [00:00<00:00,  3.61it/s]\n",
            "                 all        11        20         0         0    0.0189         0\n",
            "\n",
            "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n",
            "      5/24     8.29G      4.01      2.11     0.187      6.31         4       512: 100% 11/11 [00:08<00:00,  1.26it/s]\n",
            "               Class    Images   Targets         P         R   mAP@0.5        F1: 100% 2/2 [00:00<00:00,  4.64it/s]\n",
            "                 all        11        20    0.0669    0.0556    0.0436    0.0607\n",
            "\n",
            "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n",
            "      6/24     8.29G      4.12      2.06     0.173      6.35         4       512: 100% 11/11 [00:08<00:00,  1.26it/s]\n",
            "               Class    Images   Targets         P         R   mAP@0.5        F1: 100% 2/2 [00:00<00:00,  4.47it/s]\n",
            "                 all        11        20    0.0968     0.111    0.0656     0.103\n",
            "\n",
            "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n",
            "      7/24     8.29G      3.77      2.08      0.17      6.02         5       512: 100% 11/11 [00:08<00:00,  1.27it/s]\n",
            "               Class    Images   Targets         P         R   mAP@0.5        F1: 100% 2/2 [00:00<00:00,  4.66it/s]\n",
            "                 all        11        20    0.0546    0.0556     0.043    0.0551\n",
            "\n",
            "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n",
            "      8/24     8.29G      4.26      2.18     0.161       6.6         1       512: 100% 11/11 [00:08<00:00,  1.28it/s]\n",
            "               Class    Images   Targets         P         R   mAP@0.5        F1: 100% 2/2 [00:00<00:00,  4.45it/s]\n",
            "                 all        11        20         0         0    0.0223         0\n",
            "\n",
            "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n",
            "      9/24     8.29G       4.2      1.88     0.158      6.23         2       512: 100% 11/11 [00:08<00:00,  1.24it/s]\n",
            "               Class    Images   Targets         P         R   mAP@0.5        F1: 100% 2/2 [00:00<00:00,  4.64it/s]\n",
            "                 all        11        20    0.0804    0.0556    0.0397    0.0657\n",
            "\n",
            "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n",
            "     10/24     8.29G      4.09      1.89     0.159      6.14         2       512: 100% 11/11 [00:08<00:00,  1.30it/s]\n",
            "               Class    Images   Targets         P         R   mAP@0.5        F1: 100% 2/2 [00:00<00:00,  4.56it/s]\n",
            "                 all        11        20     0.317     0.111     0.105     0.165\n",
            "\n",
            "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n",
            "     11/24     8.29G      3.97      1.94     0.165      6.08         2       512: 100% 11/11 [00:08<00:00,  1.27it/s]\n",
            "               Class    Images   Targets         P         R   mAP@0.5        F1: 100% 2/2 [00:00<00:00,  3.64it/s]\n",
            "                 all        11        20       0.2    0.0556    0.0667    0.0869\n",
            "\n",
            "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n",
            "     12/24     8.29G      4.27      1.95     0.141      6.36         7       512: 100% 11/11 [00:08<00:00,  1.23it/s]\n",
            "               Class    Images   Targets         P         R   mAP@0.5        F1: 100% 2/2 [00:00<00:00,  4.59it/s]\n",
            "                 all        11        20     0.345    0.0556    0.0735    0.0957\n",
            "\n",
            "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n",
            "     13/24     8.29G      3.79      1.79     0.138      5.71         3       512: 100% 11/11 [00:08<00:00,  1.30it/s]\n",
            "               Class    Images   Targets         P         R   mAP@0.5        F1: 100% 2/2 [00:00<00:00,  4.55it/s]\n",
            "                 all        11        20     0.207     0.046    0.0651    0.0753\n",
            "\n",
            "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n",
            "     14/24     8.29G      4.05      1.76     0.136      5.95         4       512: 100% 11/11 [00:08<00:00,  1.29it/s]\n",
            "               Class    Images   Targets         P         R   mAP@0.5        F1: 100% 2/2 [00:00<00:00,  4.54it/s]\n",
            "                 all        11        20    0.0748    0.0498    0.0339    0.0598\n",
            "\n",
            "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n",
            "     15/24     8.29G      3.96      2.05     0.135      6.15         7       512: 100% 11/11 [00:08<00:00,  1.27it/s]\n",
            "               Class    Images   Targets         P         R   mAP@0.5        F1: 100% 2/2 [00:00<00:00,  4.64it/s]\n",
            "                 all        11        20    0.0813    0.0556    0.0229     0.066\n",
            "\n",
            "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n",
            "     16/24     8.29G       3.9      2.01     0.141      6.05         4       512: 100% 11/11 [00:08<00:00,  1.28it/s]\n",
            "               Class    Images   Targets         P         R   mAP@0.5        F1: 100% 2/2 [00:00<00:00,  4.57it/s]\n",
            "                 all        11        20    0.0242    0.0161     0.042    0.0194\n",
            "\n",
            "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n",
            "     17/24     8.29G      3.71      1.96     0.125      5.79         7       512: 100% 11/11 [00:08<00:00,  1.29it/s]\n",
            "               Class    Images   Targets         P         R   mAP@0.5        F1: 100% 2/2 [00:00<00:00,  4.63it/s]\n",
            "                 all        11        20     0.159    0.0556    0.0481    0.0824\n",
            "\n",
            "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n",
            "     18/24     8.29G      3.53      1.78      0.12      5.43         2       512: 100% 11/11 [00:08<00:00,  1.26it/s]\n",
            "               Class    Images   Targets         P         R   mAP@0.5        F1: 100% 2/2 [00:00<00:00,  4.64it/s]\n",
            "                 all        11        20     0.206    0.0556    0.0601    0.0875\n",
            "\n",
            "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n",
            "     19/24     8.29G      3.76      1.71     0.143      5.61         6       512: 100% 11/11 [00:08<00:00,  1.29it/s]\n",
            "               Class    Images   Targets         P         R   mAP@0.5        F1: 100% 2/2 [00:00<00:00,  4.65it/s]\n",
            "                 all        11        20     0.191    0.0556     0.064     0.086\n",
            "\n",
            "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n",
            "     20/24     8.29G      3.51      1.77     0.118      5.39         9       512: 100% 11/11 [00:08<00:00,  1.29it/s]\n",
            "               Class    Images   Targets         P         R   mAP@0.5        F1: 100% 2/2 [00:00<00:00,  3.83it/s]\n",
            "                 all        11        20     0.217    0.0556     0.067    0.0885\n",
            "\n",
            "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n",
            "     21/24     8.29G      3.24       1.7     0.129      5.07         3       512: 100% 11/11 [00:08<00:00,  1.26it/s]\n",
            "               Class    Images   Targets         P         R   mAP@0.5        F1: 100% 2/2 [00:00<00:00,  4.62it/s]\n",
            "                 all        11        20         0         0    0.0758         0\n",
            "\n",
            "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n",
            "     22/24     8.29G      2.93      1.66     0.118      4.71         5       512: 100% 11/11 [00:08<00:00,  1.28it/s]\n",
            "               Class    Images   Targets         P         R   mAP@0.5        F1: 100% 2/2 [00:00<00:00,  4.59it/s]\n",
            "                 all        11        20    0.0575    0.0255    0.0757    0.0354\n",
            "\n",
            "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n",
            "     23/24     8.29G      3.28      1.67     0.132      5.08         3       512: 100% 11/11 [00:08<00:00,  1.28it/s]\n",
            "               Class    Images   Targets         P         R   mAP@0.5        F1: 100% 2/2 [00:00<00:00,  3.05it/s]\n",
            "                 all        11        20     0.103    0.0456    0.0792    0.0632\n",
            "\n",
            "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n",
            "     24/24     8.29G      3.04      1.69     0.122      4.85         2       512: 100% 11/11 [00:08<00:00,  1.27it/s]\n",
            "               Class    Images   Targets         P         R   mAP@0.5        F1: 100% 2/2 [00:00<00:00,  4.61it/s]\n",
            "                 all        11        20     0.112    0.0824    0.0872     0.095\n",
            "25 epochs completed in 0.066 hours.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pwd\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "yRXbF02gkOpR",
        "outputId": "5fe1878b-ae89-4038-e40d-e1558e77dff6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/tsai_yolo/YoloV3-master'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import torch\n",
        "from models import Darknet\n",
        "from utils.utils import non_max_suppression, scale_coords\n",
        "from utils.datasets import letterbox\n",
        "from PIL import Image\n",
        "\n",
        "def load_model(cfg_path, weights_path):\n",
        "    # Load YOLOv3 model architecture\n",
        "    model = Darknet(cfg_path)\n",
        "    # Load weights\n",
        "    model.load_state_dict(torch.load(weights_path, map_location=torch.device('cpu'))['model'])\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "def detect_objects(image_path, model, conf_thres=0.3, iou_thres=0.6, img_size=416):\n",
        "    img = cv2.imread(image_path)\n",
        "    # Resize image using letterbox resize\n",
        "    img = letterbox(img, img_size, 416)[0]\n",
        "    # Convert image to RGB and then to a torch tensor\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    img = torch.from_numpy(img.transpose(2, 0, 1)).float().div(255.0).unsqueeze(0)\n",
        "    # Perform inference\n",
        "    with torch.no_grad():\n",
        "        detections = model(img)\n",
        "        detections = non_max_suppression(detections, conf_thres, iou_thres)\n",
        "    return detections[0]\n",
        "\n",
        "def draw_boxes(image_path, detections, class_names):\n",
        "    img = cv2.imread(image_path)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    if detections is not None:\n",
        "        # Rescale boxes to original image size\n",
        "        detections = scale_coords(detections, 416, img.shape[:2])\n",
        "        for x1, y1, x2, y2, conf, cls_conf, cls_pred in detections:\n",
        "            box_w = x2 - x1\n",
        "            box_h = y2 - y1\n",
        "            color = (0, 255, 0)  # Green color for bounding box\n",
        "            label = f'{class_names[int(cls_pred)]}: {conf:.2f}'\n",
        "            cv2.rectangle(img, (x1, y1), (x2, y2), color, 2)\n",
        "            cv2.putText(img, label, (x1, y1 - 8), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
        "    # Convert image back to BGR for displaying with OpenCV\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
        "    return img\n",
        "\n",
        "# Path to your custom YOLOv3 config file\n",
        "cfg_path = '/content/drive/MyDrive/tsai_yolo/YoloV3-master/cfg/yolov3-custom.cfg'\n",
        "# Path to your custom YOLOv3 weights file\n",
        "weights_path = '/content/drive/MyDrive/tsai_yolo/YoloV3-master/weights/last.pt'\n",
        "# Path to the image you want to detect objects in\n",
        "image_path = r'data/dataset/images/0025.jpg'\n",
        "# Path to your custom class names file\n",
        "class_names_path = 'data/dataset/custom.names'\n",
        "\n",
        "# Load the model\n",
        "model = load_model(cfg_path, weights_path)\n",
        "\n",
        "# Load class names\n",
        "class_names = []\n",
        "with open(class_names_path, 'r') as file:\n",
        "    class_names = file.read().splitlines()\n",
        "\n",
        "# Detect objects in the image\n",
        "detections = detect_objects(image_path, model)\n",
        "\n",
        "# Draw bounding boxes and labels on the image\n",
        "output_image = draw_boxes(image_path, detections, class_names)\n",
        "\n",
        "# Display the output image\n",
        "# cv2.imshow('Object Detection', output_image)\n",
        "# cv2.waitKey(0)\n",
        "# cv2.destroyAllWindows()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "id": "xT51nDyBiFez",
        "outputId": "d5e25955-03e7-407d-a330-f8041338ed09"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: smart bias initialization failure.\n",
            "WARNING: smart bias initialization failure.\n",
            "WARNING: smart bias initialization failure.\n",
            "Model Summary: 225 layers, 6.29987e+07 parameters, 6.29987e+07 gradients\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "shape '[1, 3, 6, 13, 13]' is invalid for input of size 43095",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-40161133361f>\u001b[0m in \u001b[0;36m<cell line: 64>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;31m# Detect objects in the image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m \u001b[0mdetections\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetect_objects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;31m# Draw bounding boxes and labels on the image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-33-40161133361f>\u001b[0m in \u001b[0;36mdetect_objects\u001b[0;34m(image_path, model, conf_thres, iou_thres, img_size)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# Perform inference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mdetections\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mdetections\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnon_max_suppression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf_thres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miou_thres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdetections\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/tsai_yolo/YoloV3-master/models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, augment, verbose)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0maugment\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Augment images (inference and test only) https://github.com/ultralytics/yolov3/issues/931\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m             \u001b[0mimg_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# height, width\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/tsai_yolo/YoloV3-master/models.py\u001b[0m in \u001b[0;36mforward_once\u001b[0;34m(self, x, augment, verbose)\u001b[0m\n\u001b[1;32m    285\u001b[0m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# WeightedFeatureFusion(), FeatureConcat()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'YOLOLayer'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m                 \u001b[0myolo_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# run module directly, i.e. mtype = 'convolutional', 'upsample', 'maxpool', 'batchnorm2d' etc.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/tsai_yolo/YoloV3-master/models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, p, out)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;31m# p.view(bs, 255, 13, 13) -- > (bs, 3, 13, 13, 85)  # (bs, anchors, grid, grid, classes + xywh)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: shape '[1, 3, 6, 13, 13]' is invalid for input of size 43095"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "cv2_imshow(output_image)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "id": "TGePvnJoiP8b",
        "outputId": "123457d3-3a48-413b-cabb-77f42ed2252a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'output_image' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-59424f5ea7cf>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatches\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcv2_imshow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcv2_imshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'output_image' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python detect.py --cfg cfg/yolov3-custom.cfg --source data/dataset/test --output /content/drive/MyDrive/tsai_yolo/YoloV3-master/output --weights weights/last.pt\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LiR9rBA6kq5_",
        "outputId": "0935ad3d-9270-40b6-fc53-e17dc62f023e"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(cfg='cfg/yolov3-custom.cfg', names='data/customdata/custom.names', weights='weights/last.pt', source='data/dataset/test', output='/content/drive/MyDrive/tsai_yolo/YoloV3-master/output', img_size=512, conf_thres=0.3, iou_thres=0.6, fourcc='mp4v', half=False, device='', view_img=False, save_txt=False, classes=None, agnostic_nms=False, augment=False)\n",
            "Using CUDA device0 _CudaDeviceProperties(name='Tesla T4', total_memory=15102MB)\n",
            "\n",
            "Model Summary: 225 layers, 6.29987e+07 parameters, 6.29987e+07 gradients\n",
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:507: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3549.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/tsai_yolo/YoloV3-master/detect.py\", line 186, in <module>\n",
            "    detect()\n",
            "  File \"/content/drive/MyDrive/tsai_yolo/YoloV3-master/detect.py\", line 78, in detect\n",
            "    _ = model(torch.zeros((1, 3, img_size, img_size), device=device)) if device.type != 'cpu' else None  # run once\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/drive/MyDrive/tsai_yolo/YoloV3-master/models.py\", line 235, in forward\n",
            "    return self.forward_once(x)\n",
            "  File \"/content/drive/MyDrive/tsai_yolo/YoloV3-master/models.py\", line 287, in forward_once\n",
            "    yolo_out.append(module(x, out))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/content/drive/MyDrive/tsai_yolo/YoloV3-master/models.py\", line 188, in forward\n",
            "    p = p.view(bs, self.na, self.no, self.ny, self.nx).permute(0, 1, 3, 4, 2).contiguous()  # prediction\n",
            "RuntimeError: shape '[1, 3, 6, 16, 16]' is invalid for input of size 65280\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "from models import *\n",
        "from utils.utils import *\n",
        "\n",
        "# Define the function for object detection\n",
        "def detect(model, img_path, img_size=512, conf_thres=0.3, iou_thres=0.6, device=''):\n",
        "    # Load image\n",
        "    img0 = cv2.imread(img_path)  # BGR\n",
        "    img = letterbox(img0, new_shape=img_size)[0]\n",
        "\n",
        "    # Normalize image\n",
        "    img = img[:, :, ::-1].transpose(2, 0, 1)  # BGR to RGB, HWC to CHW\n",
        "    img = np.ascontiguousarray(img)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "      device = torch.device('cuda')\n",
        "    else:\n",
        "      device = torch.device('cpu')\n",
        "\n",
        "    # Convert image to tensor\n",
        "    img = torch.from_numpy(img).to(device)\n",
        "    img = img.float()  # uint8 to fp16/32\n",
        "    img /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
        "\n",
        "    if img.ndimension() == 3:\n",
        "        img = img.unsqueeze(0)\n",
        "\n",
        "    # Inference\n",
        "    pred = model(img)[0]\n",
        "\n",
        "    # Apply NMS\n",
        "    pred = non_max_suppression(pred, conf_thres, iou_thres)\n",
        "\n",
        "    # Process detections\n",
        "    for i, det in enumerate(pred):  # detections per image\n",
        "        if det is not None and len(det):\n",
        "            # Rescale boxes from 416 to image size\n",
        "            det[:, :4] = scale_coords(img.shape[2:], det[:, :4], img0.shape).round()\n",
        "\n",
        "            # Draw bounding boxes and labels\n",
        "            for *xyxy, conf, cls in det:\n",
        "                label = f'{names[int(cls)]} {conf:.2f}'\n",
        "                plot_one_box(xyxy, img0, label=label, color=colors[int(cls)])\n",
        "\n",
        "    return img0\n",
        "\n",
        "# Path to your custom YOLOv3 weights file\n",
        "weights_path = '/content/drive/MyDrive/tsai_yolo/YoloV3-master/weights/last.pt'\n",
        "# Path to the image you want to detect objects in\n",
        "image_path = r'data/dataset/images/0025.jpg'\n",
        "\n",
        "# Load model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = Darknet(cfg='cfg/yolov3-custom.cfg', img_size=512)\n",
        "model.load_state_dict(torch.load(weights_path, map_location=device)['model'])\n",
        "model.to(device).eval()\n",
        "\n",
        "# Run detection\n",
        "output_image = detect(model, image_path)\n",
        "\n",
        "# Display the output image\n",
        "cv2_imshow(output_image)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "id": "x8ybeXp5ynF0",
        "outputId": "012bc348-ec78-430e-98be-8b1cfb03493c"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: smart bias initialization failure.\n",
            "WARNING: smart bias initialization failure.\n",
            "WARNING: smart bias initialization failure.\n",
            "Model Summary: 225 layers, 6.29987e+07 parameters, 6.29987e+07 gradients\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "shape '[1, 3, 6, 16, 16]' is invalid for input of size 65280",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-51fb739ba337>\u001b[0m in \u001b[0;36m<cell line: 66>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;31m# Run detection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m \u001b[0moutput_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;31m# Display the output image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-36-51fb739ba337>\u001b[0m in \u001b[0;36mdetect\u001b[0;34m(model, img_path, img_size, conf_thres, iou_thres, device)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;31m# Inference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;31m# Apply NMS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/tsai_yolo/YoloV3-master/models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, augment, verbose)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0maugment\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Augment images (inference and test only) https://github.com/ultralytics/yolov3/issues/931\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m             \u001b[0mimg_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# height, width\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/tsai_yolo/YoloV3-master/models.py\u001b[0m in \u001b[0;36mforward_once\u001b[0;34m(self, x, augment, verbose)\u001b[0m\n\u001b[1;32m    285\u001b[0m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# WeightedFeatureFusion(), FeatureConcat()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'YOLOLayer'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m                 \u001b[0myolo_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# run module directly, i.e. mtype = 'convolutional', 'upsample', 'maxpool', 'batchnorm2d' etc.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/tsai_yolo/YoloV3-master/models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, p, out)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;31m# p.view(bs, 255, 13, 13) -- > (bs, 3, 13, 13, 85)  # (bs, anchors, grid, grid, classes + xywh)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: shape '[1, 3, 6, 16, 16]' is invalid for input of size 65280"
          ]
        }
      ]
    }
  ]
}